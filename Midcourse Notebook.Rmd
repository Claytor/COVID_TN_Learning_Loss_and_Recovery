---
title: "Midcourse Notebook"
output: html_notebook
---

# Import the libraries that we need to make the stuff work.
```{r Library Imports}
library(ggplot2)
library(tidyr)
library(tidyverse)
library(dplyr)
library(readr)
library(sf)
library(stringr)
library(data.table)
library(janitor)
```

# Import the Data as CSV with readr
```{r .csv imports}
district_2018 <- read_csv("data/district/district_2018.csv")
district_2019 <- read_csv("data/district/district_2019.csv")
district_2021 <- read_csv("data/district/district_2021.csv")
district_2022 <- read_csv("data/district/district_2022.csv")
```

# Look at it with the janitor. It appears as if some of the criteria for mastery have changed over the years. 
```{r groundskeeper_willie}
groundskeeper_willie <- janitor::compare_df_cols(district_2018, district_2019, district_2021, district_2022)
groundskeeper_willie
```

# I reached out to DOE, They explained which columns are equivalent.
```{r Combined all datasets into one}
assessments_Pre <- bind_rows(district_2018, district_2019, district_2021) %>% 
  rename(n_met_expectations = n_on_track, 
         pct_met_expectations = pct_on_track,
         n_exceeded_expectations = n_mastered,
         pct_exceeded_expectations = pct_mastered,
         student_group = subgroup)
assessments_Merged <- bind_rows(assessments_Pre, district_2022)
```

# Dropping Unneeded Columns
```{r Column remove}
assessments_Drop <- assessments_Merged %>% 
  select(-c( n_below, n_approaching, n_met_expectations, n_exceeded_expectations, participation_rate, enrolled, tested, valid_tests))
```

#Describing columns

```{r Categorical Variables}
dist_assess <- assessments %>% distinct(grade, subject, test)
dist_assess <- assessments %>% distinct(grade, subject, test)
```

#All student_group all grade
```{r}
```


```{r}
assessments_All <- 
  assessments_Drop z%>% 
    filter(student_group == "All Students" & grade == "All Grades")
```

