---
title: "Midcourse Notebook"
output: html_notebook
---

# Import the libraries that we need to make the stuff work.
```{r Library Imports}
library(ggplot2)
library(tidyr)
library(tidyverse)
library(dplyr)
library(readr)
library(sf)
library(stringr)
library(data.table)
library(janitor)
```

# Import the Data as CSV with readr
```{r .csv imports}
district_2018 <- read_csv("data/district/district_2018.csv")
district_2019 <- read_csv("data/district/district_2019.csv")
district_2021 <- read_csv("data/district/district_2021.csv")
district_2022 <- read_csv("data/district/district_2022.csv")
```

# Look at it with the janitor. It appears as if some of the criteria for mastery have changed over the years. 
```{r groundskeeper_willie}
groundskeeper_willie <- janitor::compare_df_cols(district_2018, district_2019, district_2021, district_2022)
groundskeeper_willie
```

# I reached out to DOE, They explained which columns are equivalent.
```{r Combined all datasets into one}
assessments_pre <- bind_rows(district_2018, district_2019, district_2021) %>% 
  rename(n_met_expectations = n_on_track, 
         pct_met_expectations = pct_on_track,
         n_exceeded_expectations = n_mastered,
         pct_exceeded_expectations = pct_mastered,
         student_group = subgroup,
         pct_met_exceeded = pct_on_mastered)
assessments_merged <- bind_rows(assessments_pre, district_2022)
```

# Dropping Unneeded Columns
```{r Column remove}
assessments_dropped <- assessments_merged %>% 
  select(-c( n_below, n_approaching, n_met_expectations, n_exceeded_expectations, participation_rate, enrolled, tested, valid_tests))
```

#Removing Student Demographic information 
```{r filtered for all students in all grades. Removed "MSAA/Alt-Science/Social Studies" test}
assessments <- assessments_dropped %>% 
  filter(student_group == "All Students", grade == "All Grades") %>% 
  select(-c(grade, student_group))
```

#Describing columns
```{r Categorical Variables}
systems <- assessments %>% distinct(system_name)
test_subjects <- assessments %>% distinct(test, subject)

```

#Filter Dataset for suppressed scores
```{r All Suppressed Scores}
suppression_all <- assessments %>% 
  filter(pct_below %in% c("*", "**"), 
        pct_approaching %in% c("*", "**"),
        pct_met_expectations %in% c("*", "**"),
        pct_exceeded_expectations %in% c("*", "**")) %>%
  group_by(system_name, test)
```

#Looking for Assessments with high suppression rates for rates of score
reporting.  If one subject is suppressed, all will be.
```{r Suppressed completion rate}
suppression_completion <- suppression_all %>% 
  filter(pct_below %in% c("*")) %>% 
  transmute(test, subject) %>% 
  group_by(subject, .groups = test) %>% 
  summarise(count = n())
```

#Suppression for Proficiency Outliess
```{r Suppressed Outliers x<1% or x>99%}
suppression_outliers <- suppression_all %>% 
  filter(pct_below %in% c("**")) %>% 
  transmute(system_name) %>% 
  group_by(system_name) %>% 
  summarise(count = n())
```


```{r}
summary <- suppression_completion %>%
    group_by(system_name) %>%
    summarize(count_distinct = n_distinct(subject))
```

#Filter out Tests and Districts
```{r}
 #!test == "MSAA/Alt-Science/Social Studies"
```


